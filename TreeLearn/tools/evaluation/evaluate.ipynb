{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This notebook shows how to obtain a more fine-grained assessment of the tree detection and segmentation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tree_learn.util import load_data, juxtapose, plot_evaluation_results_segments\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(os.path.abspath(\"__file__\")))\n",
    "gt_forest_path = '../../data/benchmark/L1W_voxelized01_for_eval.laz' # path to ground truth forest used for evaluation\n",
    "pred_forest_path = '../../data/pipeline/L1W/results/full_forest/evaluation/pred_forest_propagated_to_gt_pointcloud.laz' # path to predicted forest (with points propagated to GT point cloud to enable visualizations)\n",
    "evaluation_results_path = '../../data/pipeline/L1W/results/full_forest/evaluation/evaluation_results.pt' # path to instance evaluation object (change according to where your evaluation was performed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = torch.load(evaluation_results_path)\n",
    "gt_forest = load_data(gt_forest_path)\n",
    "pred_forest = load_data(pred_forest_path)\n",
    "\n",
    "coords = gt_forest[:, :3]\n",
    "instance_labels = gt_forest[:, 3]\n",
    "instance_preds = pred_forest[:, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance detection\n",
    "Apart from looking at the aggregated metrics displayed in the log, it is also possible to display more fine-grained information about the non-matched predictions or ground truths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non-matched predictions\n",
    "print(f'non-matched predictions: {evaluation_results[\"detection_results\"][\"non_matched_preds_filtered\"]}') \n",
    "print(f'corresponding ground truth of non-matched predictions: {evaluation_results[\"detection_results\"][\"non_matched_preds_corresponding_gt_filtered\"]}')\n",
    "\n",
    "# non-matched ground truths\n",
    "print(f'non-matched ground truths: {evaluation_results[\"detection_results\"][\"non_matched_gts\"]}')\n",
    "print(f'corresponding prediction of non-matched ground truths: {evaluation_results[\"detection_results\"][\"non_matched_gts_corresponding_pred\"]}')\n",
    "print(f'matched ground truth tree for that prediction: {evaluation_results[\"detection_results\"][\"non_matched_gts_corresponding_other_tree\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these label correspondences, it is possible to visualize concrete cases of omission and comission errors. For example, in the code below, change the pred_num and gt_num to a pair taken from \"non-matched prediction\" and \"corresponding ground truth of non-matched predictions\" to visualize commission errors. You can toggle the visualization of the juxtaposed point clouds by clicking on the legend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_num = 62\n",
    "gt_num = 190\n",
    "coords_pred = coords[instance_preds == pred_num]\n",
    "coords_gt = coords[instance_labels == gt_num]\n",
    "juxtapose(coords_pred, coords_gt, 'pred', 'gt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance segmentation\n",
    "Apart from the aggregated metrics displayed in the log, it is possible to look at the segmentation of individual trees:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results['segmentation_results']['no_partition']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, you can visualize the results of the xy and z partition evaluation. You need to adjust the parameters of the plot according to the results of your method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = '#010003'\n",
    "fontsize=7\n",
    "fig_size=(4,1)\n",
    "y_range=[60, 100]\n",
    "x_label = \"Center \\u2192 Outer branches\"\n",
    "fig, axs = plt.subplots(1, 3, figsize=fig_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# precision\n",
    "values = evaluation_results['segmentation_results']['xy_partition'].iloc[:, 2:12].mean(0).to_numpy() * 100\n",
    "axs[0] = plot_evaluation_results_segments(axs[0], values, fontsize=fontsize, measure=\"Precision in %\", y_range=y_range, color=color, x_label=x_label)\n",
    "\n",
    "# recall\n",
    "values = evaluation_results['segmentation_results']['xy_partition'].iloc[:, 12:22].mean(0).to_numpy() * 100\n",
    "axs[1] = plot_evaluation_results_segments(axs[1], values, fontsize=fontsize, measure=\"Recall in %\", y_range=y_range, color=color, x_label=x_label)\n",
    "\n",
    "# coverage\n",
    "values = evaluation_results['segmentation_results']['xy_partition'].iloc[:, 22:32].mean(0).to_numpy() * 100\n",
    "axs[2] = plot_evaluation_results_segments(axs[2], values, fontsize=fontsize, measure=\"Coverage in %\", y_range=y_range, color=color, x_label=x_label)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.45, left=0, right=1, bottom=0.15, top=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = '#010003'\n",
    "fontsize=7\n",
    "fig_size=(4,1)\n",
    "y_range=[60, 100]\n",
    "x_label = \"Bottom \\u2192 Top\"\n",
    "fig, axs = plt.subplots(1, 3, figsize=fig_size)\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# precision\n",
    "values = evaluation_results['segmentation_results']['z_partition'].iloc[:, 2:12].mean(0).to_numpy() * 100\n",
    "axs[0] = plot_evaluation_results_segments(axs[0], values, fontsize=fontsize, measure=\"Precision in %\", y_range=y_range, color=color, x_label=x_label)\n",
    "\n",
    "# recall\n",
    "values = evaluation_results['segmentation_results']['z_partition'].iloc[:, 12:22].mean(0).to_numpy() * 100\n",
    "axs[1] = plot_evaluation_results_segments(axs[1], values, fontsize=fontsize, measure=\"Recall in %\", y_range=y_range, color=color, x_label=x_label)\n",
    "\n",
    "# coverage\n",
    "values = evaluation_results['segmentation_results']['z_partition'].iloc[:, 22:32].mean(0).to_numpy() * 100\n",
    "axs[2] = plot_evaluation_results_segments(axs[2], values, fontsize=fontsize, measure=\"Coverage in %\", y_range=y_range, color=color, x_label=x_label)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.45, left=0, right=1, bottom=0.15, top=1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TreeLearn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
